#undef NDEBUG

#include "llvm-version.hpp"
#include "ResolveCallees.hpp"

#include <cassert>
#include <llvm/IR/Constants.h>
#include <llvm/IR/DataLayout.h>
#include <llvm/IR/DerivedTypes.h>
#include <llvm/IR/Instructions.h>
#include <llvm/IR/Module.h>
#include <llvm/Pass.h>
#include <llvm/Support/Regex.h>

using namespace llvm;


/*
 * sample C++ source code fragment:
 *
 *   struct Base {
 *     virtual void alpha();
 *     virtual void beta();
 *     short code;
 *     char id;
 *   };
 *
 * sample vtable for above class using Clang 2.9:
 *
 *     @_ZTV4Base = constant [3 x i8*] [
 *       i8* null,
 *       i8* bitcast (%0* @_ZTI4Base to i8*),
 *       i8* bitcast (void (%struct.Base*)* @_ZN4Base5alphaEv to i8*),
 *       i8* bitcast (void (%struct.Base*)* @_ZN4Base4betaEv to i8*
 *     ]
 *
 * sample vtable for above class using llvm-gcc with DragonEgg 2.9:
 *
 *   %0 = type { i32 (...)*, i32 (...)*, i32 (%struct.Base*)* }
 *   @_ZTV4Base = weak_odr constant %0 {
 *     i32 (...)* null,
 *     i32 (...)* bitcast (%struct.__class_type_info_pseudo* @_ZTI4Base to i32 (...)*),
 *     i32 (%struct.Base*)* @_ZN4Base5alphaEv
 *   }, align 16
 */


// pattern for names of global "variables" which are actually vtables
static Regex vtableMangledNamePattern("^_ZTV[0-9]");


// first vtable slot (counting from 0) which contains method pointers
//
// Slots before this contain other class metadata.  All slots from
// this one through the end of the vtable are method pointers.
enum { VTableFirstMethodSlot = 2 };


bool
ResolveCallees::runOnModule(Module &module)
{
  // recognize vtables as global variables with special names
  for (Module::const_global_iterator global = module.global_begin(), end = module.global_end(); global != end; ++global)
    if (vtableMangledNamePattern.match(global->getName()))
      {
	// iterate over vtable slots, but skip initial slots that do
	// not have method pointers
	const Constant &initializer = cast<Constant>(*global->getInitializer());
	Constant::const_op_iterator begin = initializer.op_begin() + VTableFirstMethodSlot;
	Constant::const_op_iterator end = initializer.op_end();

	// reserve space in our index up through the last slot number
	const size_t slots = end - begin;
	if (vtablesBySlot.size() < slots)
	  vtablesBySlot.resize(slots);

	// collect methods in any vtable at slot #0, slot #1, etc.
	for (Constant::const_op_iterator element = begin; element != end; ++element)
	  {
	    // DragonEgg 2.9 directly embeds function pointers in vtable slots
	    Function *method = dyn_cast<Function>(*element);
	    if (!method)
	      {
		// Clang 2.9 bitcasts the function pointers in vtable slots
		const ConstantExpr &constExpr = cast<ConstantExpr>(**element);
		assert(constExpr.getOpcode() == Instruction::BitCast);
		method = cast<Function>(constExpr.getOperand(0));
	      }

	    const size_t slot = element - begin;
	    vtablesBySlot.at(slot).insert(method);
	  }
      }

  return false;
}


static bool
resolveDirect(const CallInst &callInst, ResolveCallees::FunctionSet &callees)
{
  // direct, statically-bound call to a unique callee
  Function * const direct = callInst.getCalledFunction();
  if (direct)
    {
      callees.clear();
      if (!direct->getName().startswith("llvm.dbg."))
	callees.insert(direct);
      return true;
    }

  return false;
}


////////////////////////////////////////////////////////////////////////
//
//  Pattern matching for virtual method calls
//


/*
 * We recognize vtable lookup using a fairly detailed pattern match,
 * mostly encoded below as a chain of dynamically-checked casts
 * (dyn_cast).  If any cast fails, then we are not looking at a vtable
 * lookup, so we return false.  If the entire pattern does match, then
 * we resolve the call to the set of possible callee methods appearing
 * in the targeted vtable slot.
 *
 * Consider the following sample C++ source code fragment, assuming
 * the above declaration of struct Base has already been seen:
 *
 *   Base *base;
 *   ...
 *   base->beta();
 *
 * For each pattern-matching method that follows, inline comments show
 * the bitcode generated by a particular front end for the above C++
 * source code.
 */


bool
ResolveCallees::resolveVirtualClang(const Value &callee, const LoadInst &loadThisInst, unsigned &slot) const
{
  /*
   * sample bitcode for above virtual call generated by Clang 2.9:
   *
   *   %2 = load %struct.Base** %1, align 8
   *   %3 = bitcast %struct.Base* %2 to void (%struct.Base*)***
   *   %4 = load void (%struct.Base*)*** %3
   *   %5 = getelementptr inbounds void (%struct.Base*)** %4, i64 1
   *   %6 = load void (%struct.Base*)** %5
   *   call void %6(%struct.Base* %2)
   */

  // %6 = load void (%struct.Base*)** %5
  const LoadInst * const loadCalleeInst = dyn_cast<LoadInst>(&callee);
  if (!loadCalleeInst)
    return false;

  // %5 = getelementptr inbounds void (%struct.Base*)** %4, i64 1
  const GetElementPtrInst * const getElemInst = dyn_cast<GetElementPtrInst>(loadCalleeInst->getPointerOperand());
  if (!(getElemInst && getElemInst->isInBounds() && getElemInst->getNumIndices() == 1))
    return false;
  const ConstantInt * const offset = dyn_cast<ConstantInt>(getElemInst->getOperand(1));
  if (!offset)
    return false;
  slot = offset->getLimitedValue();
  if (slot >= vtablesBySlot.size())
    return false;

  // %4 = load void (%struct.Base*)*** %3
  const LoadInst * const loadVTableInst = dyn_cast<LoadInst>(getElemInst->getPointerOperand());
  if (!loadVTableInst)
    return false;

  // %3 = bitcast %struct.Base* %2 to void (%struct.Base*)***
  const BitCastInst * const castInst = dyn_cast<BitCastInst>(loadVTableInst->getPointerOperand());
  if (!(castInst && castInst->getOperand(0) == &loadThisInst))
    return false;

  return true;
}


bool
ResolveCallees::resolveVirtualDragonEgg(const Value &callee, const LoadInst &loadThisInst, unsigned &slot) const
{
  /*
   * sample bitcode for above virtual call generated by llvm-gcc with
   * DragonEgg 2.9:
   *
   *   %0 = load %struct.Base** %base_addr, align 64
   *   ...
   *   %1 = getelementptr inbounds %struct.Base* %0, i32 0, i32 0
   *   %2 = load i32 (...)*** %1, align 8
   *   %3 = bitcast i32 (...)** %2 to i8*
   *   %4 = getelementptr i8* %3, i64 8
   *   %5 = bitcast i8* %4 to i32 (...)**
   *   %6 = load i32 (...)** %5, align 8
   *   %7 = bitcast i32 (...)* %6 to void (%struct.Base*)*
   *   call void %7(%struct.Base* %0)
   */

  // %7 = bitcast i32 (...)* %6 to void (%struct.Base*)*
  const BitCastInst * const castCalleeInst = dyn_cast<BitCastInst>(&callee);
  if (!castCalleeInst)
    return false;

  // %6 = load i32 (...)** %5, align 8
  const LoadInst * const loadCalleeInst = dyn_cast<LoadInst>(castCalleeInst->getOperand(0));
  if (!loadCalleeInst)
    return false;

  // %5 = bitcast i8* %4 to i32 (...)**
  const BitCastInst * const castSlotInst = dyn_cast<BitCastInst>(loadCalleeInst->getPointerOperand());
  if (!castSlotInst)
    return false;

  // %4 = getelementptr i8* %3, i64 8
  const GetElementPtrInst * const slotOffsetInst = dyn_cast<GetElementPtrInst>(castSlotInst->getOperand(0));
  if (!(slotOffsetInst && slotOffsetInst->getNumIndices() == 1))
    return false;
  const ConstantInt * const offset = dyn_cast<ConstantInt>(slotOffsetInst->getOperand(1));
  if (!offset)
    return false;
  const uint64_t offsetInBytes = offset->getLimitedValue();
#if LLVM_VERSION >= 30500
  const DataLayout &dataLayout = getAnalysis<DataLayoutPass>().getDataLayout();
#else
  const DataLayout &dataLayout = getAnalysis<DataLayout>();
#endif
  const unsigned pointerSize = dataLayout.getPointerSize();
  if (offsetInBytes % pointerSize != 0)
    return false;
  slot = offsetInBytes / pointerSize;
  if (slot >= vtablesBySlot.size())
    return false;

  // %3 = bitcast i32 (...)** %2 to i8*
  const BitCastInst * const castVTableInst = dyn_cast<BitCastInst>(slotOffsetInst->getPointerOperand());
  if (!castVTableInst)
    return false;

  // %2 = load i32 (...)*** %1, align 8
  const LoadInst * const loadVTableInst = dyn_cast<LoadInst>(castVTableInst->getOperand(0));
  if (!loadVTableInst)
    return false;

  // %1 = getelementptr inbounds %struct.Base* %0, i32 0, i32 0
  const GetElementPtrInst * const offsetVTableInst = dyn_cast<GetElementPtrInst>(loadVTableInst->getPointerOperand());
  if (!(offsetVTableInst && offsetVTableInst->getPointerOperand() == &loadThisInst))
    return false;
  if (!(offsetVTableInst->getNumIndices() == 2 && offsetVTableInst->hasAllZeroIndices()))
    return false;

  return true;
}


bool
ResolveCallees::resolveVirtualDragonEggSlot0(const Value &callee, const LoadInst &loadThisInst, unsigned &slot) const
{
  /*
   * special case bitcode generated by llvm-gcc with DragonEgg 2.9 if
   * the virtual method being called is in slot #0 of the vtable:
   *
   *   %0 = load %struct.Base** %base_addr, align 64
   *   ...
   *   %1 = getelementptr inbounds %struct.Base* %0, i32 0, i32 0
   *   %2 = load i32 (...)*** %1, align 8
   *   %3 = load i32 (...)** %2, align 8
   *   %4 = bitcast i32 (...)* %3 to void (%struct.Base*)*
   *   call void %4(%struct.Base* %0)
   */

  // %4 = bitcast i32 (...)* %3 to void (%struct.Base*)*
  const BitCastInst * const castCalleeInst = dyn_cast<BitCastInst>(&callee);
  if (!castCalleeInst)
    return false;

  // %3 = load i32 (...)** %2, align 8
  const LoadInst * const loadCalleeInst = dyn_cast<LoadInst>(castCalleeInst->getOperand(0));
  if (!loadCalleeInst)
    return false;

  // %2 = load i32 (...)*** %1, align 8
  const LoadInst * const loadVTableInst = dyn_cast<LoadInst>(loadCalleeInst->getOperand(0));
  if (!loadVTableInst)
    return false;

  // %1 = getelementptr inbounds %struct.Base* %0, i32 0, i32 0
  const GetElementPtrInst * const offsetVTableInst = dyn_cast<GetElementPtrInst>(loadVTableInst->getPointerOperand());
  if (!(offsetVTableInst && offsetVTableInst->getPointerOperand() == &loadThisInst))
    return false;
  if (!(offsetVTableInst->getNumIndices() == 2 && offsetVTableInst->hasAllZeroIndices()))
    return false;

  slot = 0;
  return true;
}


bool
ResolveCallees::resolveVirtual(const CallInst &callInst, FunctionSet &callees) const
{
  // virtual method calls always have a "this" argument
  if (callInst.getNumArgOperands() == 0)
    return false;

  // "this" argument always results from a memory load
  const LoadInst * const loadThisInst = dyn_cast<LoadInst>(callInst.getArgOperand(0));
  if (!loadThisInst)
    return false;

  // "this" is always a pointer to a struct
  if (const PointerType * const thisPointerType = dyn_cast<PointerType>(loadThisInst->getType()))
    if (!isa<StructType>(thisPointerType->getElementType()))
      return false;

  // several front ends to recognize; stop as soon as one succeeds
  const Value &callee = *callInst.getCalledValue();
  unsigned slot;
  if (resolveVirtualDragonEggSlot0(callee, *loadThisInst, slot) ||
      resolveVirtualDragonEgg(callee, *loadThisInst, slot) ||
      resolveVirtualClang(callee, *loadThisInst, slot))
    {
      callees = vtablesBySlot.at(slot);
      return true;
    }
  else
    return false;
}


static bool
resolveAlias(const CallInst &, ResolveCallees::FunctionSet &)
{
  // indirect call via function pointer; currently not implemented
  return false;
}


bool
ResolveCallees::resolveIndirect(const CallInst &callInst, FunctionSet &callees) const
{
  assert(!callInst.getCalledFunction());

  // several strategies to try; stop as soon as one succeeds
  return
    resolveVirtual(callInst, callees) ||
    resolveAlias(callInst, callees);
}


bool
ResolveCallees::resolve(const CallInst &callInst, FunctionSet &callees) const
{
  // several strategies to try; stop as soon as one succeeds
  return
    resolveDirect(callInst, callees) ||
    resolveIndirect(callInst, callees);
}


void
ResolveCallees::getAnalysisUsage(AnalysisUsage &usage) const
{
  usage.setPreservesAll();
#if LLVM_VERSION >= 30500
  usage.addRequired<DataLayoutPass>();
#else
  usage.addRequired<DataLayout>();
#endif
}


char ResolveCallees::ID;
static const RegisterPass<ResolveCallees> registration("resolve-callees", "Identify possible callees at indirect call sites");
